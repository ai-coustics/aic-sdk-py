{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install numpy scipy matplotlib soundfile tqdm pyfar aic-sdk ipywidgets\n",
    "import os\n",
    "import numpy as np\n",
    "import pyfar as pf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import aic_sdk as aic\n",
    "import scipy\n",
    "\n",
    "MODELS_PATH = \"aic_models\"\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "ai_coustics_models = {\n",
    "    # L/16kHz models:\n",
    "    \"sparrow-l-16khz\": 16000,\n",
    "    \"quail-vf-l-16khz\": 16000,\n",
    "    \"quail-l-16khz\": 16000,\n",
    "\n",
    "    # \"sparrow-s-16khz\": 16000,\n",
    "    # \"quail-s-16khz\": 16000,\n",
    "\n",
    "    # \"quail-l-8khz\": 8000,\n",
    "    # \"quail-s-8khz\": 8000,\n",
    "    # \"sparrow-s-8khz\": 8000,\n",
    "    # \"sparrow-l-8khz\": 8000,\n",
    "\n",
    "    # \"sparrow-l-48khz\": 48000,\n",
    "    # \"sparrow-s-48khz\": 48000,\n",
    "    # \"sparrow-xs-48khz\": 48000,\n",
    "    # \"sparrow-xxs-48khz\": 48000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('ðŸ‘‰ <a href=\"https://developers.ai-coustics.io/dashboard/sdk/keys\" target=\"_blank\">Generate your AIC_SDK_LICENSE here</a>'))\n",
    "\n",
    "# Use Textarea instead of Text\n",
    "license_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Paste your AIC_SDK_LICENSE here',\n",
    "    description='License:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='50%', height='40px') # Adjusted height\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Set License\",\n",
    "    button_style='primary'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        if license_widget.value:\n",
    "            os.environ[\"AIC_SDK_LICENSE\"] = license_widget.value.strip()\n",
    "            license_widget.value = \"\" # Clear for security\n",
    "            license_widget.placeholder = \"License key saved in memory\"\n",
    "            print(\"âœ… AIC_SDK_LICENSE has been set successfully.\")\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "display(widgets.HBox([license_widget, submit_button]), output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_chunk(\n",
    "    processor: aic.ProcessorAsync,\n",
    "    vad_ctx: aic.VadContext,\n",
    "    chunk: np.ndarray,\n",
    "    buffer_size: int,\n",
    "    num_channels: int,\n",
    ") -> tuple[np.ndarray, bool] :\n",
    "    \"\"\"Process a single audio chunk with the given processor.\"\"\"\n",
    "    valid_samples = chunk.shape[1]\n",
    "\n",
    "    # Create and zero-initialize process buffer\n",
    "    process_buffer = np.zeros((num_channels, buffer_size), dtype=np.float32)\n",
    "\n",
    "    # Copy input data into the buffer\n",
    "    process_buffer[:, :valid_samples] = chunk\n",
    "\n",
    "    # Process the chunk\n",
    "    processed_chunk = await processor.process_async(process_buffer)\n",
    "\n",
    "    # Return only the valid part\n",
    "    return processed_chunk[:, :valid_samples], vad_ctx.is_speech_detected()\n",
    "\n",
    "async def process_single(\n",
    "    audio_input: np.ndarray,\n",
    "    sample_rate: int,\n",
    "    num_channels: int,\n",
    "    enhancement_level: float | None,\n",
    "    model_id: str,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    # check if model is in folder\n",
    "    available_models = os.listdir(MODELS_PATH)\n",
    "    for model in available_models:\n",
    "        if model_id.replace(\"-\", \"_\") in model:\n",
    "            model_path = os.path.join(MODELS_PATH, model)\n",
    "            model = aic.Model.from_file(model_path)\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Downloading model {model_id}...\")\n",
    "        model_path = aic.Model.download(model_id, \"models\")\n",
    "        model = aic.Model.from_file(model_path)\n",
    "\n",
    "    # Configure generic processor (handles buffering internally if needed)\n",
    "    config = aic.ProcessorConfig.optimal(\n",
    "        model, sample_rate=sample_rate, num_channels=num_channels\n",
    "    )\n",
    "    \n",
    "    # Create the processor\n",
    "    processor = aic.ProcessorAsync(\n",
    "        model, \n",
    "        license_key=os.getenv(\"AIC_SDK_LICENSE\"), \n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Get Contexts\n",
    "    proc_ctx = processor.get_processor_context()\n",
    "    vad_ctx = processor.get_vad_context()\n",
    "\n",
    "    # Set parameters\n",
    "    if enhancement_level is not None:\n",
    "        try:\n",
    "            proc_ctx.set_parameter(aic.ProcessorParameter.EnhancementLevel, enhancement_level)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # --- LATENCY COMPENSATION LOGIC ---\n",
    "    \n",
    "    # 2. Get the latency (in samples)\n",
    "    latency_samples = proc_ctx.get_output_delay()\n",
    "\n",
    "    # 3. Pad the INPUT at the END\n",
    "    # We add zeros to the end to flush the model's internal buffer\n",
    "    padding = np.zeros((num_channels, latency_samples), dtype=np.float32)\n",
    "    padded_input = np.concatenate([audio_input, padding], axis=1)\n",
    "\n",
    "    # Prepare outputs (same size as padded input)\n",
    "    output_buffer = np.zeros_like(padded_input)\n",
    "    vad_buffer = np.zeros_like(padded_input)\n",
    "\n",
    "    # 4. Process Loop\n",
    "    num_frames = config.num_frames\n",
    "    total_len = padded_input.shape[1]\n",
    "\n",
    "    for start in range(0, total_len, num_frames):\n",
    "        end = min(start + num_frames, total_len)\n",
    "        chunk_len = end - start\n",
    "        \n",
    "        # Extract chunk\n",
    "        chunk = padded_input[:, start:end]\n",
    "\n",
    "        # Handle last chunk padding (if smaller than model frame size)\n",
    "        if chunk_len < num_frames:\n",
    "            chunk = np.pad(chunk, ((0,0), (0, num_frames - chunk_len)))\n",
    "\n",
    "        # Process\n",
    "        processed_chunk, is_speech = await process_chunk(\n",
    "            processor, vad_ctx, chunk, num_frames, num_channels\n",
    "        )\n",
    "\n",
    "        # Store result (cropping back if we padded the chunk itself)\n",
    "        output_buffer[:, start:end] = processed_chunk[:, :chunk_len]\n",
    "        vad_buffer[:, start:end] = 1.0 if is_speech else 0.0\n",
    "\n",
    "    # 5. Crop the OUTPUT at the START\n",
    "    # The first 'latency_samples' are garbage/warm-up. \n",
    "    # The actual aligned audio follows immediately after.\n",
    "    final_audio = output_buffer[:, latency_samples:]\n",
    "    final_vad = vad_buffer[:, latency_samples:]\n",
    "\n",
    "    return final_audio, final_vad\n",
    "\n",
    "def generate_test_signals(target_fs):\n",
    "    f_arr = pf.signals.files.speech(voice=\"female\", sampling_rate=target_fs).time\n",
    "    f_arr = np.pad(f_arr, ((0, 0), (0, target_fs * 2)))\n",
    "    m_arr = pf.signals.files.speech(voice=\"male\", sampling_rate=target_fs).time\n",
    "    m_arr = np.pad(m_arr, ((0, 0), (target_fs * 2, 0)))\n",
    "    d_arr = pf.signals.files.drums(sampling_rate=target_fs).time\n",
    "    r_arr = pf.signals.files.room_impulse_response(sampling_rate=target_fs).time\n",
    "\n",
    "    target_len = f_arr.shape[1]\n",
    "\n",
    "    def force_len(sig, length):\n",
    "        \"\"\"Truncates or zero-pads signal to match specific length.\"\"\"\n",
    "        current = sig.shape[1]\n",
    "        if current > length:\n",
    "            return sig[:, :length]\n",
    "        elif current < length:\n",
    "            return np.pad(sig, ((0, 0), (0, length - current)))\n",
    "        return sig\n",
    "\n",
    "    d_resized = force_len(d_arr, target_len)\n",
    "    female_drums = f_arr + d_resized\n",
    "    m_conv = scipy.signal.fftconvolve(m_arr, r_arr, mode=\"full\")\n",
    "    m_conv *= 0.1\n",
    "    m_conv_resized = force_len(m_conv, target_len)\n",
    "    female_male = f_arr + m_conv_resized\n",
    "    return female_drums, female_male\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "async def run_demo():\n",
    "    print(f\"Starting processing for {len(ai_coustics_models)} models...\\n\")\n",
    "\n",
    "    for i, (model_id, fs) in enumerate(ai_coustics_models.items()):\n",
    "\n",
    "        # Section Header\n",
    "        display(HTML(f\"<h2>Model: {model_id} ({fs} Hz)</h2>\"))\n",
    "\n",
    "        # 1. Generate Signals\n",
    "        female_drums, female_male = generate_test_signals(fs)\n",
    "\n",
    "        # 2. Display Input Audio (Before Processing)\n",
    "        print(\"Input Signal (Noisy):\")\n",
    "        display(Audio(female_drums[0], rate=fs))\n",
    "        display(Audio(female_male[0], rate=fs))\n",
    "\n",
    "        # 3. Process\n",
    "        output_sig_drums, vad_vector_drums = await process_single(\n",
    "            audio_input=female_drums.astype(np.float32),\n",
    "            sample_rate=fs,\n",
    "            num_channels=1,\n",
    "            enhancement_level=1.0,\n",
    "            model_id=model_id,\n",
    "        )\n",
    "\n",
    "        output_sig_female_male, vad_vector_fm = await process_single(\n",
    "            audio_input=female_male.astype(np.float32),\n",
    "            sample_rate=fs,\n",
    "            num_channels=1,\n",
    "            enhancement_level=1.0,\n",
    "            model_id=model_id,\n",
    "        )\n",
    "\n",
    "        # 4. Plotting (One plot per model)\n",
    "        _, ax = plt.subplots(figsize=(14, 4))\n",
    "        duration = female_drums.shape[1] / fs\n",
    "        time_axis = np.linspace(0, duration, female_drums.shape[1])\n",
    "        ax.plot(time_axis, female_drums[0], label=\"Original (Noisy)\", alpha=0.6, color=\"silver\") # Corrected plotting dimension\n",
    "        ax.plot(time_axis, output_sig_drums[0], label=\"Enhanced (Processed)\", alpha=0.9, color=\"#007acc\", linewidth=1) # Corrected plotting dimension\n",
    "        ax.plot(time_axis, vad_vector_drums[0] * np.max(output_sig_drums[0]), label=\"VAD Output\", linestyle=\"--\", color=\"red\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.set_xlim(0, duration)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        _, ax = plt.subplots(figsize=(14, 4))\n",
    "        duration = female_male.shape[1] / fs\n",
    "        time_axis = np.linspace(0, duration, female_male.shape[1])\n",
    "        ax.plot(time_axis, female_male[0], label=\"Original (Noisy)\", alpha=0.6, color=\"silver\") # Corrected plotting dimension\n",
    "        ax.plot(time_axis, output_sig_female_male[0], label=\"Enhanced (Processed)\", alpha=0.9, color=\"#007acc\", linewidth=1) # Corrected plotting dimension\n",
    "        ax.plot(time_axis, vad_vector_fm[0] * np.max(output_sig_female_male[0]), label=\"VAD Output\", linestyle=\"--\", color=\"red\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.set_xlim(0, duration)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 5. Display Output Audio (After Processing)\n",
    "        print(\"Output Signal (Enhanced):\")\n",
    "        display(Audio(output_sig_drums[0], rate=fs))\n",
    "        display(Audio(output_sig_female_male[0], rate=fs))\n",
    "\n",
    "\n",
    "        # Separator\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "await run_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aic-sdk-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
